<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Aditya</title>
  
  <subtitle>silverf3lix</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://adityapadala.com/"/>
  <updated>2020-04-20T20:11:38.857Z</updated>
  <id>http://adityapadala.com/</id>
  
  <author>
    <name>Aditya Vardhan Padala</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Working with libFuzzer</title>
    <link href="http://adityapadala.com/2020/04/21/Working-with-libFuzzer/"/>
    <id>http://adityapadala.com/2020/04/21/Working-with-libFuzzer/</id>
    <published>2020-04-20T20:11:38.000Z</published>
    <updated>2020-04-20T20:11:38.857Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Setting Up NetBSD Kernel Dev Environment</title>
    <link href="http://adityapadala.com/2020/04/20/Setting-Up-NetBSD-Kernel-Dev-Environment/"/>
    <id>http://adityapadala.com/2020/04/20/Setting-Up-NetBSD-Kernel-Dev-Environment/</id>
    <published>2020-04-20T17:39:42.000Z</published>
    <updated>2020-04-20T19:01:25.196Z</updated>
    
    <content type="html"><![CDATA[<p>I used T_PAGEFLT’s blog post as a reference for setting my NetBSD kernel development environment since his website is down I’m putting down the steps here so it would be helpful for starters.</p><p>This is an overview of my setup:</p><ol><li>Linux Host With Qemu Target</li><li>Tracing and Debugging using qemu’s built-in gdb server.</li><li>Use a cronjob with rsync to keep my files updated b/w host and guest.</li><li>pkgin for simpicity.(Sometimes have to use pkg_add to get stuff done)</li></ol><h2 id="Host-Configuration"><a href="#Host-Configuration" class="headerlink" title="Host Configuration"></a>Host Configuration</h2><p>Make sure you have the latest version of qemu installed as we will be using x86-64 NetBSD guests.</p><p>We will be needing GDB that is configured with NetBSD x86_64 abi. So we need to compile it ourself.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">wget http://ftp.gnu.org/gnu/gdb/gdb-xxxx.tar.xz</span><br><span class="line">tar xvf gdb-xxxx.tar.xz</span><br><span class="line">sudo mkdir -p /opt &amp;&amp; cd gdb-xxxx</span><br><span class="line">./configure --prefix=/opt --target=x86_64-netbsd</span><br><span class="line">make -j4 &amp;&amp; sudo make install</span><br></pre></td></tr></table></figure><h3 id="Building-from-NetBSD-current"><a href="#Building-from-NetBSD-current" class="headerlink" title="Building from NetBSD-current"></a>Building from NetBSD-current</h3><p>First step get the files.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir netbsd &amp;&amp; cd netbsd</span><br><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;NetBSD&#x2F;src</span><br><span class="line">cd src</span><br></pre></td></tr></table></figure><p>Now for the time taking part, compiling the sources.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;build.sh -m amd64 -T ..&#x2F;tooldir -D ..&#x2F;destdir -R ..&#x2F;releasedir -O ..&#x2F;objdir -U -j6 release iso-image</span><br><span class="line"># Now get some sleep it&#39;ll take some time.</span><br></pre></td></tr></table></figure><p>Upon completion the directories will have the following files:</p><ul><li>Cross-compilation toolchain in “tooldir”</li><li>Bootable image in “releasedir/images”</li></ul><h2 id="Now-getting-the-guest-up-and-running"><a href="#Now-getting-the-guest-up-and-running" class="headerlink" title="Now getting the guest up and running"></a>Now getting the guest up and running</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">qemu-img create ~/vhd/netbsd-current.img 10G</span><br><span class="line">qemu-system-x86_64 -smp 4 -drive file=vhd/netbsd-current.img,format=raw \</span><br><span class="line">                     -cdrom ~/Code/netbsd-current/releasedir/images/NetBSD-8.99.12-amd64.iso\</span><br><span class="line">     -m 2048 -enable-kvm \</span><br><span class="line">     -net user,hostfwd=tcp::5022-:22 -n</span><br></pre></td></tr></table></figure><p>No go through the standard installation without unnecessary clutter like x11,games etc.<br>Configure all the necessary things such as ssh, users, sheel preference.<br>After the completion of installation</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">qemu-system-x86_64 -smp 4 -drive file=vhd/netbsd-current.img,format=raw \</span><br><span class="line">                     -m 2048 -enable-kvm \</span><br><span class="line">     -net user,hostfwd=tcp::5022-:22 -net nic</span><br></pre></td></tr></table></figure><p>This should drop you a vm instance.</p><h3 id="Installing-pkgin"><a href="#Installing-pkgin" class="headerlink" title="Installing pkgin"></a>Installing pkgin</h3><p>Once you are up and running better to install pkgin. It makes package management easier.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">su -</span><br><span class="line">export PKG_URL="http://cdn.netbsd.org/pub/pkgsrc/packages/NetBSD/amd64/9.0_2019Q4/All"</span><br><span class="line"><span class="meta">#</span><span class="bash"> change the above url accordingly</span></span><br><span class="line">pkg_add "$PKG_URL/pkgin-0.9.4nb6.tgz"</span><br><span class="line">echo $PKG_URL &gt; /usr/pkg/etc/pkgin/repositories.conf</span><br><span class="line">pkgin update</span><br><span class="line">``` </span><br><span class="line">Now we can install all the necessary utilities that makes life easier.</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Compiling kernels</span></span></span><br><span class="line"></span><br><span class="line">NetBSD runs the default GENERIC configuration. So we make a few changes to this and compile our own kernel.</span><br><span class="line">```shell</span><br><span class="line">cd ~/netbsd/src/sys/arch/amd64/conf</span><br><span class="line">cp GENERIC QEMU</span><br></pre></td></tr></table></figure><p>Use the text editor of your choice and fiddle with the configuration.</p><p>make sure you have</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">makeoptions     DEBUG="-g"      # compile full symbol table for CTF</span><br></pre></td></tr></table></figure><p>Now all that is left is compiling.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">./build.sh -m amd64 -T ../tooldir -D ../destdir -R ../releasedir -O ../objdir -U -u -j6 kernel=QEMU</span><br><span class="line"><span class="meta">#</span><span class="bash"> This should complete fairly quick</span></span><br></pre></td></tr></table></figure><p>Now scp the files to vm and repace the old kernel</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scp -P 5022 ~/netbsd/objdir/sys/arch/amd64/compile/QEMU/netbsd root@localhost:~</span><br><span class="line">ssh -p 5022 root@localhost</span><br><span class="line"><span class="meta">#</span><span class="bash"> We forwared VM<span class="string">'s port 22 --&gt; Host 5022 for ssh acceess</span></span></span><br><span class="line">cp /netbsd /neetbsd.old</span><br><span class="line">cp ~/netbsd /netbsd</span><br><span class="line"><span class="meta">#</span><span class="bash"> Reboot with the new kernel</span></span><br></pre></td></tr></table></figure><h3 id="Debugging-with-gdb"><a href="#Debugging-with-gdb" class="headerlink" title="Debugging with gdb"></a>Debugging with gdb</h3><p>Start VM with qemu’s gdb stub forwarding tcp through port 1234.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">qemu-system-x86_64 -drive file&#x3D;vhd&#x2F;netbsd-current.img,format&#x3D;raw \</span><br><span class="line">                     -m 1024 -enable-kvm \</span><br><span class="line">    -gdb tcp::1234</span><br></pre></td></tr></table></figure><p>We already compiled netbsd kernel with complete symbol table.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd ~&#x2F;netbsd&#x2F;objdir&#x2F;sys&#x2F;arch&#x2F;amd64&#x2F;compile&#x2F;QEMU</span><br><span class="line">&#x2F;opt&#x2F;bin&#x2F;x86_64-netbsd-gdb .&#x2F;netbsd.gdb</span><br></pre></td></tr></table></figure><p>Now just simply enter</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(gdb) target remote localhost:1234</span><br><span class="line">Remote debugging using localhost:1234</span><br><span class="line">0xffffffff8021d16e in x86_stihlt ()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;I used T_PAGEFLT’s blog post as a reference for setting my NetBSD kernel development environment since his website is down I’m putting do
      
    
    </summary>
    
    
    
      <category term="netbsd, environment" scheme="http://adityapadala.com/tags/netbsd-environment/"/>
    
  </entry>
  
  <entry>
    <title>Rump Kernels</title>
    <link href="http://adityapadala.com/2020/04/01/Rump-Kernels/"/>
    <id>http://adityapadala.com/2020/04/01/Rump-Kernels/</id>
    <published>2020-04-01T05:34:45.000Z</published>
    <updated>2020-04-01T07:37:15.344Z</updated>
    
    <content type="html"><![CDATA[<p>The following blogpost is a summary of <a href="http://rumpkernel.org/misc/usenix-login-2014/login_1410_03_kantee.pdf" target="_blank" rel="noopener">Rump Kernels: No OS? No Problem</a>.</p><p>Rumpkernels provide all the necessary components to run applications on baremetal without the necessity of an operating system. Simply put it is way to run kernel code in user space.</p><p>The main goal of rumpkernels in netbsd is to run,debug,examine and develop kernel drivers as easy as possible in the user space without having to run the entire kernel but run the exact same kernel code in userspace. This makes most of the components(drivers) easily portable to different environments.</p><p>Rump Kernels are constructed out of components, So the drivers are built as libraries and these libraries are linked to an interface(some application) that makes use of the libraries(drivers). So we need not build the entire monolithic kernel just the required parts of the kernel.</p><p>For example if we are running a web server all we need is a tcp/ip stack and sockets, We don’t need memory manager, file systems. To achieve this goal we need to find a way to scrape the drivers from the kernel code and must facilitate the rump kernel with i/o device access, memory etc.Here comes the anykernel and hypercall interface.</p><h3 id="Anykernel"><a href="#Anykernel" class="headerlink" title="Anykernel"></a>Anykernel</h3><p>This is the core concept in the implementation of the rumpkernel. Anykernel is using “any” driver/s in any configuration(monolithic/micro/exo). It is analogous to loading kernel modules into any place beyond the operating system.</p><p>The anykernel is divided into 3 abstractions:</p><ul><li>base-Contains fundamental routines(allocators, sync routines)</li><li>factions- filesystem, i/o devices, networking</li><li>drivers-actual driver code to use the factions.</li></ul><p>Consider NFS(Network File System) which is half file system and half network protocol, in order to construct a rump kernel consisting the necessary drivers we must also build they dependent-drivers. But in cases where rumpkernel differs from monolithic kernel we must use some “glue code” to make sure thigns run properly while making sure that the glue code is minimal so as to assure maintainability on NetBSD.</p><h3 id="Hypercalls"><a href="#Hypercalls" class="headerlink" title="Hypercalls"></a>Hypercalls</h3><p>For proper operation of the rump kernel we require require resourcs such as i/o functions and memory. These resources are facilitated by the hypercall interface. It provides a bridge b/w the rumpkernel and the platform the rumpkernel is running on. So we need some bootstrap code to run on the host platform to facilitate this interface. Hypercall is a software trap from the rump kernel to the platform that the rump is running on. Hypercall is to a hypervisor what a syscall is to the kernel.</p><h1 id="Fundamental-Characteristics"><a href="#Fundamental-Characteristics" class="headerlink" title="Fundamental Characteristics"></a>Fundamental Characteristics</h1><p><strong>Rumpkernel is always executed by the host platform.</strong><br>It is similar to just running a binary on the userspace or in Xen(hypervisor used extensively to test rumpkernel not sure why?) ti is just starting a guest domain or on embedded platforms the bootloader loads the rumpkernel into memory and we just <em>jump to the entry point</em> of the rumpkernel code. Quite contrasting to how the monolithic kernel is ran either on hardware or virtualization the only difference arises when executing applications which is not natively possible on rumpkernels(but application layer can be bundled with rumpkernels). We can have different processes communicating with the rumpkernel but either way it is still linked, loaded, executed by the host patform.</p><p><strong>Notion of a CPU core is fictional</strong><br>Usually the CPU configuration is in our hands for a virtual machine or the kernel running on bare metal but this is simply not possible on the rump kernel. The number of cores is actually the number of threads running we can simply all the cpu cores to rump cores for improved performance using caching and locking.</p><p><strong>There is no scheduler</strong><br>Rumpkernels use the platforms thread scheduling policy there is no native scheduler running in rumpkernel<br>so as to avoid the overhead of running scheduler on a scheduler. All the sync ops are defines as hypercall interfaces so that they can be optimized further and avoid classic execution problems(spinlocks/deadlocks). Since there is scheduling policy the host is free to schedule/unschedule the running thread as required.</p><p><strong>No virtual memory concept</strong><br>The rumpkernel simpy runs in the space allocated to it either virtual or not. This is just to remove the cumbersome work of porting the complex memory management operations and the memory manager itself to the rumpkernel when they are explicitly not required. But there are cases where we might need to implement a few custom alternatives to achieve memory manager dependent tasks(mmap()).</p><h3 id="Machine-Independence"><a href="#Machine-Independence" class="headerlink" title="Machine Independence"></a>Machine Independence</h3><p>The rumpkernel code can virtually run on any platform due to their implementation but the only limitation is the driver size so we need enough RAM/ROM to load the rump kernel but can be achieved on a lower scale by trimming down the driver code which obviously ruins the purpose of rumpkernel but still it is possible. What blew my mind is that they were able to achieve the goal of runnibg the rumpkernel on a Javascript Engines.</p><h3 id="Virtual-Uniprocessor-and-Memory-Lock-Avoidance"><a href="#Virtual-Uniprocessor-and-Memory-Lock-Avoidance" class="headerlink" title="Virtual Uniprocessor and Memory Lock Avoidance"></a>Virtual Uniprocessor and Memory Lock Avoidance</h3><p>We can completely avoid the concept of memory locks by using just one processor core in the rumpkernel. So the locking scheme can be implemented in a single file without modifying the driver code.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;The following blogpost is a summary of &lt;a href=&quot;http://rumpkernel.org/misc/usenix-login-2014/login_1410_03_kantee.pdf&quot; target=&quot;_blank&quot; re
      
    
    </summary>
    
    
    
      <category term="rump, kernel, rumpkernel, netbsd" scheme="http://adityapadala.com/tags/rump-kernel-rumpkernel-netbsd/"/>
    
  </entry>
  
</feed>
